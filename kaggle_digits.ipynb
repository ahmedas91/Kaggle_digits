{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my test post, we'll solve  Kaggle's [Digit Recognizer](https://www.kaggle.com/c/digit-recognizer) competition using python's machine learning library `sklearn`. It a really simple problem and used as a starting point (along with the [Titanic](https://www.kaggle.com/c/titanic) one) Kaggle competitions. If you want to check out an implementation from scratch, I have uploaded one on this repo on github. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X =  np.genfromtxt('train.csv',\n",
    "                   dtype='int_', \n",
    "                   delimiter=',', \n",
    "                   skip_header=1)\n",
    "x_test =  np.genfromtxt('test.csv',\n",
    "                        dtype='int_', \n",
    "                        delimiter=',', \n",
    "                        skip_header=1)\n",
    "x_train = X[:,1:]\n",
    "y_train = X[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the `x_train` and `x_test` data is a 28x28 pixels image with a total of 784 pixels. Therefore, we will write a simple function takes randomly selected rows, reshapes them into 28x28 matrices and display them using `matplotlib.image.mpimg`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD/CAYAAABSKwXmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSkhISEQRSXQlJCQkosj/B14m/VQCjJiqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18c803750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display(n):    \n",
    "    for i in range(1,(n**2)+1):\n",
    "        plt.subplot(n,n,i)\n",
    "        plt.axis('off')\n",
    "        pic = np.reshape(x_train[np.random.randint(1,42000)],(28,28))\n",
    "        imgplot = plt.imshow(pic, cmap='Greys')\n",
    "display(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "           random_state=None, refit=True, scoring=None, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "model_log = LogisticRegressionCV(multi_class='multinomial') \n",
    "model_log.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rf = RandomForestClassifier(1000)\n",
    "model_rf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #99, avg. train loss: 11.68955\n",
      "Step #199, avg. train loss: 1.90498\n",
      "Step #299, avg. train loss: 1.59837\n",
      "Step #399, avg. train loss: 1.39525\n",
      "Step #499, avg. train loss: 1.30917\n",
      "Step #599, avg. train loss: 1.24795\n",
      "Step #699, avg. train loss: 1.17851\n",
      "Step #799, avg. train loss: 1.14037\n",
      "Step #899, avg. train loss: 0.98428\n",
      "Step #999, avg. train loss: 0.90913\n",
      "Step #1099, avg. train loss: 0.87294\n",
      "Step #1199, avg. train loss: 0.80876\n",
      "Step #1299, avg. train loss: 0.76986\n",
      "Step #1400, epoch #1, avg. train loss: 0.66272\n",
      "Step #1500, epoch #1, avg. train loss: 0.68679\n",
      "Step #1600, epoch #1, avg. train loss: 0.71283\n",
      "Step #1700, epoch #1, avg. train loss: 0.63373\n",
      "Step #1800, epoch #1, avg. train loss: 0.62656\n",
      "Step #1900, epoch #1, avg. train loss: 0.63143\n",
      "Step #2000, epoch #1, avg. train loss: 0.59257\n",
      "Step #2100, epoch #1, avg. train loss: 0.50856\n",
      "Step #2200, epoch #1, avg. train loss: 0.54319\n",
      "Step #2300, epoch #1, avg. train loss: 0.53000\n",
      "Step #2400, epoch #1, avg. train loss: 0.53368\n",
      "Step #2500, epoch #1, avg. train loss: 0.51565\n",
      "Step #2600, epoch #1, avg. train loss: 0.53530\n",
      "Step #2700, epoch #2, avg. train loss: 0.48693\n",
      "Step #2800, epoch #2, avg. train loss: 0.40579\n",
      "Step #2900, epoch #2, avg. train loss: 0.42831\n",
      "Step #3000, epoch #2, avg. train loss: 0.42732\n",
      "Step #3100, epoch #2, avg. train loss: 0.44406\n",
      "Step #3200, epoch #2, avg. train loss: 0.43181\n",
      "Step #3300, epoch #2, avg. train loss: 0.37903\n",
      "Step #3400, epoch #2, avg. train loss: 0.42245\n",
      "Step #3500, epoch #2, avg. train loss: 0.41271\n",
      "Step #3600, epoch #2, avg. train loss: 0.40049\n",
      "Step #3700, epoch #2, avg. train loss: 0.36674\n",
      "Step #3800, epoch #2, avg. train loss: 0.42336\n",
      "Step #3900, epoch #2, avg. train loss: 0.41822\n",
      "Step #4000, epoch #3, avg. train loss: 0.39095\n",
      "Step #4100, epoch #3, avg. train loss: 0.32801\n",
      "Step #4200, epoch #3, avg. train loss: 0.36751\n",
      "Step #4300, epoch #3, avg. train loss: 0.36475\n",
      "Step #4400, epoch #3, avg. train loss: 0.32208\n",
      "Step #4500, epoch #3, avg. train loss: 0.32349\n",
      "Step #4600, epoch #3, avg. train loss: 0.35807\n",
      "Step #4700, epoch #3, avg. train loss: 0.36693\n",
      "Step #4800, epoch #3, avg. train loss: 0.34095\n",
      "Step #4900, epoch #3, avg. train loss: 0.36664\n",
      "Step #5000, epoch #3, avg. train loss: 0.34091\n",
      "Step #5100, epoch #3, avg. train loss: 0.31549\n",
      "Step #5200, epoch #3, avg. train loss: 0.37935\n",
      "Step #5300, epoch #4, avg. train loss: 0.29695\n",
      "Step #5400, epoch #4, avg. train loss: 0.30239\n",
      "Step #5500, epoch #4, avg. train loss: 0.31421\n",
      "Step #5600, epoch #4, avg. train loss: 0.28318\n",
      "Step #5700, epoch #4, avg. train loss: 0.29890\n",
      "Step #5800, epoch #4, avg. train loss: 0.32002\n",
      "Step #5900, epoch #4, avg. train loss: 0.30695\n",
      "Step #6000, epoch #4, avg. train loss: 0.31916\n",
      "Step #6100, epoch #4, avg. train loss: 0.32257\n",
      "Step #6200, epoch #4, avg. train loss: 0.27867\n",
      "Step #6300, epoch #4, avg. train loss: 0.28996\n",
      "Step #6400, epoch #4, avg. train loss: 0.28486\n",
      "Step #6500, epoch #4, avg. train loss: 0.31107\n",
      "Step #6600, epoch #5, avg. train loss: 0.27670\n",
      "Step #6700, epoch #5, avg. train loss: 0.23988\n",
      "Step #6800, epoch #5, avg. train loss: 0.26531\n",
      "Step #6900, epoch #5, avg. train loss: 0.29127\n",
      "Step #7000, epoch #5, avg. train loss: 0.26652\n",
      "Step #7100, epoch #5, avg. train loss: 0.28801\n",
      "Step #7200, epoch #5, avg. train loss: 0.27088\n",
      "Step #7300, epoch #5, avg. train loss: 0.24331\n",
      "Step #7400, epoch #5, avg. train loss: 0.22618\n",
      "Step #7500, epoch #5, avg. train loss: 0.25799\n",
      "Step #7600, epoch #5, avg. train loss: 0.24559\n",
      "Step #7700, epoch #5, avg. train loss: 0.25651\n",
      "Step #7800, epoch #5, avg. train loss: 0.28604\n",
      "Step #7900, epoch #6, avg. train loss: 0.26717\n",
      "Step #8000, epoch #6, avg. train loss: 0.24952\n",
      "Step #8100, epoch #6, avg. train loss: 0.19733\n",
      "Step #8200, epoch #6, avg. train loss: 0.25515\n",
      "Step #8300, epoch #6, avg. train loss: 0.26277\n",
      "Step #8400, epoch #6, avg. train loss: 0.22698\n",
      "Step #8500, epoch #6, avg. train loss: 0.24289\n",
      "Step #8600, epoch #6, avg. train loss: 0.25072\n",
      "Step #8700, epoch #6, avg. train loss: 0.23153\n",
      "Step #8800, epoch #6, avg. train loss: 0.27909\n",
      "Step #8900, epoch #6, avg. train loss: 0.22876\n",
      "Step #9000, epoch #6, avg. train loss: 0.20491\n",
      "Step #9100, epoch #6, avg. train loss: 0.24440\n",
      "Step #9200, epoch #7, avg. train loss: 0.23735\n",
      "Step #9300, epoch #7, avg. train loss: 0.20722\n",
      "Step #9400, epoch #7, avg. train loss: 0.22766\n",
      "Step #9500, epoch #7, avg. train loss: 0.19038\n",
      "Step #9600, epoch #7, avg. train loss: 0.22463\n",
      "Step #9700, epoch #7, avg. train loss: 0.18594\n",
      "Step #9800, epoch #7, avg. train loss: 0.21839\n",
      "Step #9900, epoch #7, avg. train loss: 0.20470\n",
      "Step #10000, epoch #7, avg. train loss: 0.20341\n",
      "Step #10100, epoch #7, avg. train loss: 0.23120\n",
      "Step #10200, epoch #7, avg. train loss: 0.20605\n",
      "Step #10300, epoch #7, avg. train loss: 0.22379\n",
      "Step #10400, epoch #7, avg. train loss: 0.21992\n",
      "Step #10500, epoch #7, avg. train loss: 0.22814\n",
      "Step #10600, epoch #8, avg. train loss: 0.15781\n",
      "Step #10700, epoch #8, avg. train loss: 0.19866\n",
      "Step #10800, epoch #8, avg. train loss: 0.20383\n",
      "Step #10900, epoch #8, avg. train loss: 0.18658\n",
      "Step #11000, epoch #8, avg. train loss: 0.16017\n",
      "Step #11100, epoch #8, avg. train loss: 0.18873\n",
      "Step #11200, epoch #8, avg. train loss: 0.17667\n",
      "Step #11300, epoch #8, avg. train loss: 0.18903\n",
      "Step #11400, epoch #8, avg. train loss: 0.21161\n",
      "Step #11500, epoch #8, avg. train loss: 0.21328\n",
      "Step #11600, epoch #8, avg. train loss: 0.18918\n",
      "Step #11700, epoch #8, avg. train loss: 0.24092\n",
      "Step #11800, epoch #8, avg. train loss: 0.19531\n",
      "Step #11900, epoch #9, avg. train loss: 0.17138\n",
      "Step #12000, epoch #9, avg. train loss: 0.18620\n",
      "Step #12100, epoch #9, avg. train loss: 0.17035\n",
      "Step #12200, epoch #9, avg. train loss: 0.16919\n",
      "Step #12300, epoch #9, avg. train loss: 0.17785\n",
      "Step #12400, epoch #9, avg. train loss: 0.15492\n",
      "Step #12500, epoch #9, avg. train loss: 0.16267\n",
      "Step #12600, epoch #9, avg. train loss: 0.16753\n",
      "Step #12700, epoch #9, avg. train loss: 0.16873\n",
      "Step #12800, epoch #9, avg. train loss: 0.18268\n",
      "Step #12900, epoch #9, avg. train loss: 0.21389\n",
      "Step #13000, epoch #9, avg. train loss: 0.19479\n",
      "Step #13100, epoch #9, avg. train loss: 0.17893\n",
      "Step #13200, epoch #10, avg. train loss: 0.17760\n",
      "Step #13300, epoch #10, avg. train loss: 0.16271\n",
      "Step #13400, epoch #10, avg. train loss: 0.17669\n",
      "Step #13500, epoch #10, avg. train loss: 0.15688\n",
      "Step #13600, epoch #10, avg. train loss: 0.17874\n",
      "Step #13700, epoch #10, avg. train loss: 0.19457\n",
      "Step #13800, epoch #10, avg. train loss: 0.14491\n",
      "Step #13900, epoch #10, avg. train loss: 0.16436\n",
      "Step #14000, epoch #10, avg. train loss: 0.17888\n",
      "Step #14100, epoch #10, avg. train loss: 0.18107\n",
      "Step #14200, epoch #10, avg. train loss: 0.16413\n",
      "Step #14300, epoch #10, avg. train loss: 0.17805\n",
      "Step #14400, epoch #10, avg. train loss: 0.14787\n",
      "Step #14500, epoch #11, avg. train loss: 0.16208\n",
      "Step #14600, epoch #11, avg. train loss: 0.16686\n",
      "Step #14700, epoch #11, avg. train loss: 0.15366\n",
      "Step #14800, epoch #11, avg. train loss: 0.13911\n",
      "Step #14900, epoch #11, avg. train loss: 0.14908\n",
      "Step #15000, epoch #11, avg. train loss: 0.15301\n",
      "Step #15100, epoch #11, avg. train loss: 0.15065\n",
      "Step #15200, epoch #11, avg. train loss: 0.14456\n",
      "Step #15300, epoch #11, avg. train loss: 0.14044\n",
      "Step #15400, epoch #11, avg. train loss: 0.16389\n",
      "Step #15500, epoch #11, avg. train loss: 0.16006\n",
      "Step #15600, epoch #11, avg. train loss: 0.16838\n",
      "Step #15700, epoch #11, avg. train loss: 0.16109\n",
      "Step #15800, epoch #12, avg. train loss: 0.14297\n",
      "Step #15900, epoch #12, avg. train loss: 0.14299\n",
      "Step #16000, epoch #12, avg. train loss: 0.15464\n",
      "Step #16100, epoch #12, avg. train loss: 0.13245\n",
      "Step #16200, epoch #12, avg. train loss: 0.16626\n",
      "Step #16300, epoch #12, avg. train loss: 0.14468\n",
      "Step #16400, epoch #12, avg. train loss: 0.13355\n",
      "Step #16500, epoch #12, avg. train loss: 0.13987\n",
      "Step #16600, epoch #12, avg. train loss: 0.15358\n",
      "Step #16700, epoch #12, avg. train loss: 0.15336\n",
      "Step #16800, epoch #12, avg. train loss: 0.15903\n",
      "Step #16900, epoch #12, avg. train loss: 0.14370\n",
      "Step #17000, epoch #12, avg. train loss: 0.15476\n",
      "Step #17100, epoch #13, avg. train loss: 0.15491\n",
      "Step #17200, epoch #13, avg. train loss: 0.11179\n",
      "Step #17300, epoch #13, avg. train loss: 0.12398\n",
      "Step #17400, epoch #13, avg. train loss: 0.16022\n",
      "Step #17500, epoch #13, avg. train loss: 0.15307\n",
      "Step #17600, epoch #13, avg. train loss: 0.17482\n",
      "Step #17700, epoch #13, avg. train loss: 0.14035\n",
      "Step #17800, epoch #13, avg. train loss: 0.15679\n",
      "Step #17900, epoch #13, avg. train loss: 0.12819\n",
      "Step #18000, epoch #13, avg. train loss: 0.13123\n",
      "Step #18100, epoch #13, avg. train loss: 0.15229\n",
      "Step #18200, epoch #13, avg. train loss: 0.13594\n",
      "Step #18300, epoch #13, avg. train loss: 0.14782\n",
      "Step #18400, epoch #14, avg. train loss: 0.13174\n",
      "Step #18500, epoch #14, avg. train loss: 0.11585\n",
      "Step #18600, epoch #14, avg. train loss: 0.13594\n",
      "Step #18700, epoch #14, avg. train loss: 0.12942\n",
      "Step #18800, epoch #14, avg. train loss: 0.12375\n",
      "Step #18900, epoch #14, avg. train loss: 0.11551\n",
      "Step #19000, epoch #14, avg. train loss: 0.14433\n",
      "Step #19100, epoch #14, avg. train loss: 0.13147\n",
      "Step #19200, epoch #14, avg. train loss: 0.15013\n",
      "Step #19300, epoch #14, avg. train loss: 0.14020\n",
      "Step #19400, epoch #14, avg. train loss: 0.14223\n",
      "Step #19500, epoch #14, avg. train loss: 0.15480\n",
      "Step #19600, epoch #14, avg. train loss: 0.14726\n",
      "Step #19700, epoch #15, avg. train loss: 0.14599\n",
      "Step #19800, epoch #15, avg. train loss: 0.14326\n",
      "Step #19900, epoch #15, avg. train loss: 0.12559\n",
      "Step #20000, epoch #15, avg. train loss: 0.11793\n",
      "Step #20100, epoch #15, avg. train loss: 0.11781\n",
      "Step #20200, epoch #15, avg. train loss: 0.12041\n",
      "Step #20300, epoch #15, avg. train loss: 0.13638\n",
      "Step #20400, epoch #15, avg. train loss: 0.13690\n",
      "Step #20500, epoch #15, avg. train loss: 0.11847\n",
      "Step #20600, epoch #15, avg. train loss: 0.09777\n",
      "Step #20700, epoch #15, avg. train loss: 0.15622\n",
      "Step #20800, epoch #15, avg. train loss: 0.11769\n",
      "Step #20900, epoch #15, avg. train loss: 0.13144\n",
      "Step #21000, epoch #15, avg. train loss: 0.13410\n",
      "Step #21100, epoch #16, avg. train loss: 0.11682\n",
      "Step #21200, epoch #16, avg. train loss: 0.11518\n",
      "Step #21300, epoch #16, avg. train loss: 0.12250\n",
      "Step #21400, epoch #16, avg. train loss: 0.12128\n",
      "Step #21500, epoch #16, avg. train loss: 0.12918\n",
      "Step #21600, epoch #16, avg. train loss: 0.11237\n",
      "Step #21700, epoch #16, avg. train loss: 0.13261\n",
      "Step #21800, epoch #16, avg. train loss: 0.13431\n",
      "Step #21900, epoch #16, avg. train loss: 0.12160\n",
      "Step #22000, epoch #16, avg. train loss: 0.12477\n",
      "Step #22100, epoch #16, avg. train loss: 0.12824\n",
      "Step #22200, epoch #16, avg. train loss: 0.14191\n",
      "Step #22300, epoch #16, avg. train loss: 0.10908\n",
      "Step #22400, epoch #17, avg. train loss: 0.12730\n",
      "Step #22500, epoch #17, avg. train loss: 0.12128\n",
      "Step #22600, epoch #17, avg. train loss: 0.11257\n",
      "Step #22700, epoch #17, avg. train loss: 0.11145\n",
      "Step #22800, epoch #17, avg. train loss: 0.11322\n",
      "Step #22900, epoch #17, avg. train loss: 0.09535\n",
      "Step #23000, epoch #17, avg. train loss: 0.12198\n",
      "Step #23100, epoch #17, avg. train loss: 0.14443\n",
      "Step #23200, epoch #17, avg. train loss: 0.12522\n",
      "Step #23300, epoch #17, avg. train loss: 0.10372\n",
      "Step #23400, epoch #17, avg. train loss: 0.09443\n",
      "Step #23500, epoch #17, avg. train loss: 0.11162\n",
      "Step #23600, epoch #17, avg. train loss: 0.13109\n",
      "Step #23700, epoch #18, avg. train loss: 0.10133\n",
      "Step #23800, epoch #18, avg. train loss: 0.11512\n",
      "Step #23900, epoch #18, avg. train loss: 0.10359\n",
      "Step #24000, epoch #18, avg. train loss: 0.09717\n",
      "Step #24100, epoch #18, avg. train loss: 0.11660\n",
      "Step #24200, epoch #18, avg. train loss: 0.10578\n",
      "Step #24300, epoch #18, avg. train loss: 0.11295\n",
      "Step #24400, epoch #18, avg. train loss: 0.11433\n",
      "Step #24500, epoch #18, avg. train loss: 0.09672\n",
      "Step #24600, epoch #18, avg. train loss: 0.11208\n",
      "Step #24700, epoch #18, avg. train loss: 0.10778\n",
      "Step #24800, epoch #18, avg. train loss: 0.11460\n",
      "Step #24900, epoch #18, avg. train loss: 0.16196\n",
      "Step #25000, epoch #19, avg. train loss: 0.10994\n",
      "Step #25100, epoch #19, avg. train loss: 0.09007\n",
      "Step #25200, epoch #19, avg. train loss: 0.09010\n",
      "Step #25300, epoch #19, avg. train loss: 0.11162\n",
      "Step #25400, epoch #19, avg. train loss: 0.11769\n",
      "Step #25500, epoch #19, avg. train loss: 0.10100\n",
      "Step #25600, epoch #19, avg. train loss: 0.14135\n",
      "Step #25700, epoch #19, avg. train loss: 0.10341\n",
      "Step #25800, epoch #19, avg. train loss: 0.11119\n",
      "Step #25900, epoch #19, avg. train loss: 0.11888\n",
      "Step #26000, epoch #19, avg. train loss: 0.08975\n",
      "Step #26100, epoch #19, avg. train loss: 0.12187\n",
      "Step #26200, epoch #19, avg. train loss: 0.09880\n",
      "Step #26300, epoch #20, avg. train loss: 0.11326\n",
      "Step #26400, epoch #20, avg. train loss: 0.08271\n",
      "Step #26500, epoch #20, avg. train loss: 0.12089\n",
      "Step #26600, epoch #20, avg. train loss: 0.10414\n",
      "Step #26700, epoch #20, avg. train loss: 0.10198\n",
      "Step #26800, epoch #20, avg. train loss: 0.10439\n",
      "Step #26900, epoch #20, avg. train loss: 0.11532\n",
      "Step #27000, epoch #20, avg. train loss: 0.10435\n",
      "Step #27100, epoch #20, avg. train loss: 0.09259\n",
      "Step #27200, epoch #20, avg. train loss: 0.11837\n",
      "Step #27300, epoch #20, avg. train loss: 0.10805\n",
      "Step #27400, epoch #20, avg. train loss: 0.09184\n",
      "Step #27500, epoch #20, avg. train loss: 0.10238\n",
      "Step #27600, epoch #21, avg. train loss: 0.08918\n",
      "Step #27700, epoch #21, avg. train loss: 0.07583\n",
      "Step #27800, epoch #21, avg. train loss: 0.09444\n",
      "Step #27900, epoch #21, avg. train loss: 0.09481\n",
      "Step #28000, epoch #21, avg. train loss: 0.10780\n",
      "Step #28100, epoch #21, avg. train loss: 0.10446\n",
      "Step #28200, epoch #21, avg. train loss: 0.11541\n",
      "Step #28300, epoch #21, avg. train loss: 0.09050\n",
      "Step #28400, epoch #21, avg. train loss: 0.10413\n",
      "Step #28500, epoch #21, avg. train loss: 0.08637\n",
      "Step #28600, epoch #21, avg. train loss: 0.10887\n",
      "Step #28700, epoch #21, avg. train loss: 0.08715\n",
      "Step #28800, epoch #21, avg. train loss: 0.12360\n",
      "Step #28900, epoch #22, avg. train loss: 0.10884\n",
      "Step #29000, epoch #22, avg. train loss: 0.09025\n",
      "Step #29100, epoch #22, avg. train loss: 0.08686\n",
      "Step #29200, epoch #22, avg. train loss: 0.07857\n",
      "Step #29300, epoch #22, avg. train loss: 0.11564\n",
      "Step #29400, epoch #22, avg. train loss: 0.07671\n",
      "Step #29500, epoch #22, avg. train loss: 0.09954\n",
      "Step #29600, epoch #22, avg. train loss: 0.08865\n",
      "Step #29700, epoch #22, avg. train loss: 0.10880\n",
      "Step #29800, epoch #22, avg. train loss: 0.09345\n",
      "Step #29900, epoch #22, avg. train loss: 0.11767\n",
      "Step #30000, epoch #22, avg. train loss: 0.08242\n",
      "Step #30100, epoch #22, avg. train loss: 0.11370\n",
      "Step #30200, epoch #23, avg. train loss: 0.12376\n",
      "Step #30300, epoch #23, avg. train loss: 0.08835\n",
      "Step #30400, epoch #23, avg. train loss: 0.09401\n",
      "Step #30500, epoch #23, avg. train loss: 0.10062\n",
      "Step #30600, epoch #23, avg. train loss: 0.09500\n",
      "Step #30700, epoch #23, avg. train loss: 0.08484\n",
      "Step #30800, epoch #23, avg. train loss: 0.08066\n",
      "Step #30900, epoch #23, avg. train loss: 0.10071\n",
      "Step #31000, epoch #23, avg. train loss: 0.09640\n",
      "Step #31100, epoch #23, avg. train loss: 0.09267\n",
      "Step #31200, epoch #23, avg. train loss: 0.09274\n",
      "Step #31300, epoch #23, avg. train loss: 0.09968\n",
      "Step #31400, epoch #23, avg. train loss: 0.10306\n",
      "Step #31500, epoch #23, avg. train loss: 0.10144\n",
      "Step #31600, epoch #24, avg. train loss: 0.10239\n",
      "Step #31700, epoch #24, avg. train loss: 0.08321\n",
      "Step #31800, epoch #24, avg. train loss: 0.06008\n",
      "Step #31900, epoch #24, avg. train loss: 0.09713\n",
      "Step #32000, epoch #24, avg. train loss: 0.07755\n",
      "Step #32100, epoch #24, avg. train loss: 0.10787\n",
      "Step #32200, epoch #24, avg. train loss: 0.06889\n",
      "Step #32300, epoch #24, avg. train loss: 0.08141\n",
      "Step #32400, epoch #24, avg. train loss: 0.09783\n",
      "Step #32500, epoch #24, avg. train loss: 0.09717\n",
      "Step #32600, epoch #24, avg. train loss: 0.08838\n",
      "Step #32700, epoch #24, avg. train loss: 0.11114\n",
      "Step #32800, epoch #24, avg. train loss: 0.09868\n",
      "Step #32900, epoch #25, avg. train loss: 0.10128\n",
      "Step #33000, epoch #25, avg. train loss: 0.08334\n",
      "Step #33100, epoch #25, avg. train loss: 0.09608\n",
      "Step #33200, epoch #25, avg. train loss: 0.07640\n",
      "Step #33300, epoch #25, avg. train loss: 0.09095\n",
      "Step #33400, epoch #25, avg. train loss: 0.09351\n",
      "Step #33500, epoch #25, avg. train loss: 0.07933\n",
      "Step #33600, epoch #25, avg. train loss: 0.07791\n",
      "Step #33700, epoch #25, avg. train loss: 0.07477\n",
      "Step #33800, epoch #25, avg. train loss: 0.08643\n",
      "Step #33900, epoch #25, avg. train loss: 0.09469\n",
      "Step #34000, epoch #25, avg. train loss: 0.09419\n",
      "Step #34100, epoch #25, avg. train loss: 0.11041\n",
      "Step #34200, epoch #26, avg. train loss: 0.07314\n",
      "Step #34300, epoch #26, avg. train loss: 0.07298\n",
      "Step #34400, epoch #26, avg. train loss: 0.07044\n",
      "Step #34500, epoch #26, avg. train loss: 0.07709\n",
      "Step #34600, epoch #26, avg. train loss: 0.09569\n",
      "Step #34700, epoch #26, avg. train loss: 0.08518\n",
      "Step #34800, epoch #26, avg. train loss: 0.09173\n",
      "Step #34900, epoch #26, avg. train loss: 0.08347\n",
      "Step #35000, epoch #26, avg. train loss: 0.09117\n",
      "Step #35100, epoch #26, avg. train loss: 0.08318\n",
      "Step #35200, epoch #26, avg. train loss: 0.06390\n",
      "Step #35300, epoch #26, avg. train loss: 0.09223\n",
      "Step #35400, epoch #26, avg. train loss: 0.11222\n",
      "Step #35500, epoch #27, avg. train loss: 0.06601\n",
      "Step #35600, epoch #27, avg. train loss: 0.06718\n",
      "Step #35700, epoch #27, avg. train loss: 0.08082\n",
      "Step #35800, epoch #27, avg. train loss: 0.06275\n",
      "Step #35900, epoch #27, avg. train loss: 0.09018\n",
      "Step #36000, epoch #27, avg. train loss: 0.08379\n",
      "Step #36100, epoch #27, avg. train loss: 0.06935\n",
      "Step #36200, epoch #27, avg. train loss: 0.07493\n",
      "Step #36300, epoch #27, avg. train loss: 0.08602\n",
      "Step #36400, epoch #27, avg. train loss: 0.09512\n",
      "Step #36500, epoch #27, avg. train loss: 0.09414\n",
      "Step #36600, epoch #27, avg. train loss: 0.10433\n",
      "Step #36700, epoch #27, avg. train loss: 0.07718\n",
      "Step #36800, epoch #28, avg. train loss: 0.08026\n",
      "Step #36900, epoch #28, avg. train loss: 0.07255\n",
      "Step #37000, epoch #28, avg. train loss: 0.06833\n",
      "Step #37100, epoch #28, avg. train loss: 0.08396\n",
      "Step #37200, epoch #28, avg. train loss: 0.08685\n",
      "Step #37300, epoch #28, avg. train loss: 0.07401\n",
      "Step #37400, epoch #28, avg. train loss: 0.06515\n",
      "Step #37500, epoch #28, avg. train loss: 0.06606\n",
      "Step #37600, epoch #28, avg. train loss: 0.05900\n",
      "Step #37700, epoch #28, avg. train loss: 0.08678\n",
      "Step #37800, epoch #28, avg. train loss: 0.08543\n",
      "Step #37900, epoch #28, avg. train loss: 0.08805\n",
      "Step #38000, epoch #28, avg. train loss: 0.09857\n",
      "Step #38100, epoch #29, avg. train loss: 0.07128\n",
      "Step #38200, epoch #29, avg. train loss: 0.06294\n",
      "Step #38300, epoch #29, avg. train loss: 0.06280\n",
      "Step #38400, epoch #29, avg. train loss: 0.08291\n",
      "Step #38500, epoch #29, avg. train loss: 0.07428\n",
      "Step #38600, epoch #29, avg. train loss: 0.07552\n",
      "Step #38700, epoch #29, avg. train loss: 0.08013\n",
      "Step #38800, epoch #29, avg. train loss: 0.08641\n",
      "Step #38900, epoch #29, avg. train loss: 0.09406\n",
      "Step #39000, epoch #29, avg. train loss: 0.06484\n",
      "Step #39100, epoch #29, avg. train loss: 0.08561\n",
      "Step #39200, epoch #29, avg. train loss: 0.07575\n",
      "Step #39300, epoch #29, avg. train loss: 0.07505\n",
      "Step #39400, epoch #30, avg. train loss: 0.06746\n",
      "Step #39500, epoch #30, avg. train loss: 0.07867\n",
      "Step #39600, epoch #30, avg. train loss: 0.05047\n",
      "Step #39700, epoch #30, avg. train loss: 0.05466\n",
      "Step #39800, epoch #30, avg. train loss: 0.07184\n",
      "Step #39900, epoch #30, avg. train loss: 0.07638\n",
      "Step #40000, epoch #30, avg. train loss: 0.09180\n",
      "Step #40100, epoch #30, avg. train loss: 0.07454\n",
      "Step #40200, epoch #30, avg. train loss: 0.07733\n",
      "Step #40300, epoch #30, avg. train loss: 0.06740\n",
      "Step #40400, epoch #30, avg. train loss: 0.08122\n",
      "Step #40500, epoch #30, avg. train loss: 0.09354\n",
      "Step #40600, epoch #30, avg. train loss: 0.06994\n",
      "Step #40700, epoch #30, avg. train loss: 0.07621\n",
      "Step #40800, epoch #31, avg. train loss: 0.05981\n",
      "Step #40900, epoch #31, avg. train loss: 0.07785\n",
      "Step #41000, epoch #31, avg. train loss: 0.05194\n",
      "Step #41100, epoch #31, avg. train loss: 0.06173\n",
      "Step #41200, epoch #31, avg. train loss: 0.08563\n",
      "Step #41300, epoch #31, avg. train loss: 0.09733\n",
      "Step #41400, epoch #31, avg. train loss: 0.05921\n",
      "Step #41500, epoch #31, avg. train loss: 0.06769\n",
      "Step #41600, epoch #31, avg. train loss: 0.07192\n",
      "Step #41700, epoch #31, avg. train loss: 0.08389\n",
      "Step #41800, epoch #31, avg. train loss: 0.08156\n",
      "Step #41900, epoch #31, avg. train loss: 0.05147\n",
      "Step #42000, epoch #31, avg. train loss: 0.07467\n",
      "Step #42100, epoch #32, avg. train loss: 0.07460\n",
      "Step #42200, epoch #32, avg. train loss: 0.07229\n",
      "Step #42300, epoch #32, avg. train loss: 0.05209\n",
      "Step #42400, epoch #32, avg. train loss: 0.04825\n",
      "Step #42500, epoch #32, avg. train loss: 0.08152\n",
      "Step #42600, epoch #32, avg. train loss: 0.07977\n",
      "Step #42700, epoch #32, avg. train loss: 0.08083\n",
      "Step #42800, epoch #32, avg. train loss: 0.08594\n",
      "Step #42900, epoch #32, avg. train loss: 0.07918\n",
      "Step #43000, epoch #32, avg. train loss: 0.06758\n",
      "Step #43100, epoch #32, avg. train loss: 0.05367\n",
      "Step #43200, epoch #32, avg. train loss: 0.07272\n",
      "Step #43300, epoch #32, avg. train loss: 0.05889\n",
      "Step #43400, epoch #33, avg. train loss: 0.05772\n",
      "Step #43500, epoch #33, avg. train loss: 0.06233\n",
      "Step #43600, epoch #33, avg. train loss: 0.05701\n",
      "Step #43700, epoch #33, avg. train loss: 0.05791\n",
      "Step #43800, epoch #33, avg. train loss: 0.07659\n",
      "Step #43900, epoch #33, avg. train loss: 0.06909\n",
      "Step #44000, epoch #33, avg. train loss: 0.07848\n",
      "Step #44100, epoch #33, avg. train loss: 0.05178\n",
      "Step #44200, epoch #33, avg. train loss: 0.07302\n",
      "Step #44300, epoch #33, avg. train loss: 0.08238\n",
      "Step #44400, epoch #33, avg. train loss: 0.07713\n",
      "Step #44500, epoch #33, avg. train loss: 0.07173\n",
      "Step #44600, epoch #33, avg. train loss: 0.05881\n",
      "Step #44700, epoch #34, avg. train loss: 0.07268\n",
      "Step #44800, epoch #34, avg. train loss: 0.04629\n",
      "Step #44900, epoch #34, avg. train loss: 0.05710\n",
      "Step #45000, epoch #34, avg. train loss: 0.05504\n",
      "Step #45100, epoch #34, avg. train loss: 0.07752\n",
      "Step #45200, epoch #34, avg. train loss: 0.06815\n",
      "Step #45300, epoch #34, avg. train loss: 0.06275\n",
      "Step #45400, epoch #34, avg. train loss: 0.06928\n",
      "Step #45500, epoch #34, avg. train loss: 0.05234\n",
      "Step #45600, epoch #34, avg. train loss: 0.07033\n",
      "Step #45700, epoch #34, avg. train loss: 0.06724\n",
      "Step #45800, epoch #34, avg. train loss: 0.07080\n",
      "Step #45900, epoch #34, avg. train loss: 0.07786\n",
      "Step #46000, epoch #35, avg. train loss: 0.05922\n",
      "Step #46100, epoch #35, avg. train loss: 0.04851\n",
      "Step #46200, epoch #35, avg. train loss: 0.06518\n",
      "Step #46300, epoch #35, avg. train loss: 0.07106\n",
      "Step #46400, epoch #35, avg. train loss: 0.04896\n",
      "Step #46500, epoch #35, avg. train loss: 0.06215\n",
      "Step #46600, epoch #35, avg. train loss: 0.05324\n",
      "Step #46700, epoch #35, avg. train loss: 0.06536\n",
      "Step #46800, epoch #35, avg. train loss: 0.06601\n",
      "Step #46900, epoch #35, avg. train loss: 0.05657\n",
      "Step #47000, epoch #35, avg. train loss: 0.07692\n",
      "Step #47100, epoch #35, avg. train loss: 0.08758\n",
      "Step #47200, epoch #35, avg. train loss: 0.06051\n",
      "Step #47300, epoch #36, avg. train loss: 0.06830\n",
      "Step #47400, epoch #36, avg. train loss: 0.05718\n",
      "Step #47500, epoch #36, avg. train loss: 0.06327\n",
      "Step #47600, epoch #36, avg. train loss: 0.07062\n",
      "Step #47700, epoch #36, avg. train loss: 0.05442\n",
      "Step #47800, epoch #36, avg. train loss: 0.04944\n",
      "Step #47900, epoch #36, avg. train loss: 0.03401\n",
      "Step #48000, epoch #36, avg. train loss: 0.06586\n",
      "Step #48100, epoch #36, avg. train loss: 0.08016\n",
      "Step #48200, epoch #36, avg. train loss: 0.06419\n",
      "Step #48300, epoch #36, avg. train loss: 0.05988\n",
      "Step #48400, epoch #36, avg. train loss: 0.07292\n",
      "Step #48500, epoch #36, avg. train loss: 0.06552\n",
      "Step #48600, epoch #37, avg. train loss: 0.07876\n",
      "Step #48700, epoch #37, avg. train loss: 0.05132\n",
      "Step #48800, epoch #37, avg. train loss: 0.05304\n",
      "Step #48900, epoch #37, avg. train loss: 0.05074\n",
      "Step #49000, epoch #37, avg. train loss: 0.05960\n",
      "Step #49100, epoch #37, avg. train loss: 0.05455\n",
      "Step #49200, epoch #37, avg. train loss: 0.06071\n",
      "Step #49300, epoch #37, avg. train loss: 0.07202\n",
      "Step #49400, epoch #37, avg. train loss: 0.06734\n",
      "Step #49500, epoch #37, avg. train loss: 0.04670\n",
      "Step #49600, epoch #37, avg. train loss: 0.06118\n",
      "Step #49700, epoch #37, avg. train loss: 0.06567\n",
      "Step #49800, epoch #37, avg. train loss: 0.06483\n",
      "Step #49900, epoch #38, avg. train loss: 0.07571\n",
      "Step #50000, epoch #38, avg. train loss: 0.04256\n",
      "Step #50100, epoch #38, avg. train loss: 0.06100\n",
      "Step #50200, epoch #38, avg. train loss: 0.05633\n",
      "Step #50300, epoch #38, avg. train loss: 0.06019\n",
      "Step #50400, epoch #38, avg. train loss: 0.05559\n",
      "Step #50500, epoch #38, avg. train loss: 0.05723\n",
      "Step #50600, epoch #38, avg. train loss: 0.06429\n",
      "Step #50700, epoch #38, avg. train loss: 0.06527\n",
      "Step #50800, epoch #38, avg. train loss: 0.06649\n",
      "Step #50900, epoch #38, avg. train loss: 0.06504\n",
      "Step #51000, epoch #38, avg. train loss: 0.05468\n",
      "Step #51100, epoch #38, avg. train loss: 0.04485\n",
      "Step #51200, epoch #38, avg. train loss: 0.06282\n",
      "Step #51300, epoch #39, avg. train loss: 0.04860\n",
      "Step #51400, epoch #39, avg. train loss: 0.06022\n",
      "Step #51500, epoch #39, avg. train loss: 0.07172\n",
      "Step #51600, epoch #39, avg. train loss: 0.05816\n",
      "Step #51700, epoch #39, avg. train loss: 0.06161\n",
      "Step #51800, epoch #39, avg. train loss: 0.07208\n",
      "Step #51900, epoch #39, avg. train loss: 0.05521\n",
      "Step #52000, epoch #39, avg. train loss: 0.04540\n",
      "Step #52100, epoch #39, avg. train loss: 0.05376\n",
      "Step #52200, epoch #39, avg. train loss: 0.06025\n",
      "Step #52300, epoch #39, avg. train loss: 0.07735\n",
      "Step #52400, epoch #39, avg. train loss: 0.06156\n",
      "Step #52500, epoch #39, avg. train loss: 0.04506\n",
      "Step #52600, epoch #40, avg. train loss: 0.04661\n",
      "Step #52700, epoch #40, avg. train loss: 0.06051\n",
      "Step #52800, epoch #40, avg. train loss: 0.06051\n",
      "Step #52900, epoch #40, avg. train loss: 0.06717\n",
      "Step #53000, epoch #40, avg. train loss: 0.04506\n",
      "Step #53100, epoch #40, avg. train loss: 0.04956\n",
      "Step #53200, epoch #40, avg. train loss: 0.06017\n",
      "Step #53300, epoch #40, avg. train loss: 0.05848\n",
      "Step #53400, epoch #40, avg. train loss: 0.04685\n",
      "Step #53500, epoch #40, avg. train loss: 0.05762\n",
      "Step #53600, epoch #40, avg. train loss: 0.05120\n",
      "Step #53700, epoch #40, avg. train loss: 0.06881\n",
      "Step #53800, epoch #40, avg. train loss: 0.05811\n",
      "Step #53900, epoch #41, avg. train loss: 0.06482\n",
      "Step #54000, epoch #41, avg. train loss: 0.04448\n",
      "Step #54100, epoch #41, avg. train loss: 0.06374\n",
      "Step #54200, epoch #41, avg. train loss: 0.04838\n",
      "Step #54300, epoch #41, avg. train loss: 0.04067\n",
      "Step #54400, epoch #41, avg. train loss: 0.07145\n",
      "Step #54500, epoch #41, avg. train loss: 0.04745\n",
      "Step #54600, epoch #41, avg. train loss: 0.06071\n",
      "Step #54700, epoch #41, avg. train loss: 0.06086\n",
      "Step #54800, epoch #41, avg. train loss: 0.05842\n",
      "Step #54900, epoch #41, avg. train loss: 0.05774\n",
      "Step #55000, epoch #41, avg. train loss: 0.05856\n",
      "Step #55100, epoch #41, avg. train loss: 0.03448\n",
      "Step #55200, epoch #42, avg. train loss: 0.07320\n",
      "Step #55300, epoch #42, avg. train loss: 0.05203\n",
      "Step #55400, epoch #42, avg. train loss: 0.04634\n",
      "Step #55500, epoch #42, avg. train loss: 0.07791\n",
      "Step #55600, epoch #42, avg. train loss: 0.04305\n",
      "Step #55700, epoch #42, avg. train loss: 0.05730\n",
      "Step #55800, epoch #42, avg. train loss: 0.04864\n",
      "Step #55900, epoch #42, avg. train loss: 0.04889\n",
      "Step #56000, epoch #42, avg. train loss: 0.05005\n",
      "Step #56100, epoch #42, avg. train loss: 0.05605\n",
      "Step #56200, epoch #42, avg. train loss: 0.05199\n",
      "Step #56300, epoch #42, avg. train loss: 0.06162\n",
      "Step #56400, epoch #42, avg. train loss: 0.05180\n",
      "Step #56500, epoch #43, avg. train loss: 0.04927\n",
      "Step #56600, epoch #43, avg. train loss: 0.04467\n",
      "Step #56700, epoch #43, avg. train loss: 0.05241\n",
      "Step #56800, epoch #43, avg. train loss: 0.03836\n",
      "Step #56900, epoch #43, avg. train loss: 0.05248\n",
      "Step #57000, epoch #43, avg. train loss: 0.05079\n",
      "Step #57100, epoch #43, avg. train loss: 0.04453\n",
      "Step #57200, epoch #43, avg. train loss: 0.05942\n",
      "Step #57300, epoch #43, avg. train loss: 0.06072\n",
      "Step #57400, epoch #43, avg. train loss: 0.05471\n",
      "Step #57500, epoch #43, avg. train loss: 0.05582\n",
      "Step #57600, epoch #43, avg. train loss: 0.06024\n",
      "Step #57700, epoch #43, avg. train loss: 0.06448\n",
      "Step #57800, epoch #44, avg. train loss: 0.04146\n",
      "Step #57900, epoch #44, avg. train loss: 0.04832\n",
      "Step #58000, epoch #44, avg. train loss: 0.04842\n",
      "Step #58100, epoch #44, avg. train loss: 0.04567\n",
      "Step #58200, epoch #44, avg. train loss: 0.04994\n",
      "Step #58300, epoch #44, avg. train loss: 0.05210\n",
      "Step #58400, epoch #44, avg. train loss: 0.06742\n",
      "Step #58500, epoch #44, avg. train loss: 0.05388\n",
      "Step #58600, epoch #44, avg. train loss: 0.05428\n",
      "Step #58700, epoch #44, avg. train loss: 0.06017\n",
      "Step #58800, epoch #44, avg. train loss: 0.04610\n",
      "Step #58900, epoch #44, avg. train loss: 0.04318\n",
      "Step #59000, epoch #44, avg. train loss: 0.04355\n",
      "Step #59100, epoch #45, avg. train loss: 0.05898\n",
      "Step #59200, epoch #45, avg. train loss: 0.05360\n",
      "Step #59300, epoch #45, avg. train loss: 0.04708\n",
      "Step #59400, epoch #45, avg. train loss: 0.04180\n",
      "Step #59500, epoch #45, avg. train loss: 0.05016\n",
      "Step #59600, epoch #45, avg. train loss: 0.04536\n",
      "Step #59700, epoch #45, avg. train loss: 0.05974\n",
      "Step #59800, epoch #45, avg. train loss: 0.05286\n",
      "Step #59900, epoch #45, avg. train loss: 0.04300\n",
      "Step #60000, epoch #45, avg. train loss: 0.03260\n",
      "Step #60100, epoch #45, avg. train loss: 0.04310\n",
      "Step #60200, epoch #45, avg. train loss: 0.04981\n",
      "Step #60300, epoch #45, avg. train loss: 0.06146\n",
      "Step #60400, epoch #46, avg. train loss: 0.06284\n",
      "Step #60500, epoch #46, avg. train loss: 0.03439\n",
      "Step #60600, epoch #46, avg. train loss: 0.05280\n",
      "Step #60700, epoch #46, avg. train loss: 0.04026\n",
      "Step #60800, epoch #46, avg. train loss: 0.04958\n",
      "Step #60900, epoch #46, avg. train loss: 0.04474\n",
      "Step #61000, epoch #46, avg. train loss: 0.05154\n",
      "Step #61100, epoch #46, avg. train loss: 0.06348\n",
      "Step #61200, epoch #46, avg. train loss: 0.04332\n",
      "Step #61300, epoch #46, avg. train loss: 0.05034\n",
      "Step #61400, epoch #46, avg. train loss: 0.05564\n",
      "Step #61500, epoch #46, avg. train loss: 0.03796\n",
      "Step #61600, epoch #46, avg. train loss: 0.05861\n",
      "Step #61700, epoch #46, avg. train loss: 0.05514\n",
      "Step #61800, epoch #47, avg. train loss: 0.04006\n",
      "Step #61900, epoch #47, avg. train loss: 0.03221\n",
      "Step #62000, epoch #47, avg. train loss: 0.06384\n",
      "Step #62100, epoch #47, avg. train loss: 0.05455\n",
      "Step #62200, epoch #47, avg. train loss: 0.03720\n",
      "Step #62300, epoch #47, avg. train loss: 0.05089\n",
      "Step #62400, epoch #47, avg. train loss: 0.05352\n",
      "Step #62500, epoch #47, avg. train loss: 0.04297\n",
      "Step #62600, epoch #47, avg. train loss: 0.05139\n",
      "Step #62700, epoch #47, avg. train loss: 0.05706\n",
      "Step #62800, epoch #47, avg. train loss: 0.04686\n",
      "Step #62900, epoch #47, avg. train loss: 0.06545\n",
      "Step #63000, epoch #47, avg. train loss: 0.04641\n",
      "Step #63100, epoch #48, avg. train loss: 0.03998\n",
      "Step #63200, epoch #48, avg. train loss: 0.05537\n",
      "Step #63300, epoch #48, avg. train loss: 0.04259\n",
      "Step #63400, epoch #48, avg. train loss: 0.04619\n",
      "Step #63500, epoch #48, avg. train loss: 0.04898\n",
      "Step #63600, epoch #48, avg. train loss: 0.04960\n",
      "Step #63700, epoch #48, avg. train loss: 0.04140\n",
      "Step #63800, epoch #48, avg. train loss: 0.04400\n",
      "Step #63900, epoch #48, avg. train loss: 0.04302\n",
      "Step #64000, epoch #48, avg. train loss: 0.05148\n",
      "Step #64100, epoch #48, avg. train loss: 0.04933\n",
      "Step #64200, epoch #48, avg. train loss: 0.04801\n",
      "Step #64300, epoch #48, avg. train loss: 0.04689\n",
      "Step #64400, epoch #49, avg. train loss: 0.05247\n",
      "Step #64500, epoch #49, avg. train loss: 0.04497\n",
      "Step #64600, epoch #49, avg. train loss: 0.03923\n",
      "Step #64700, epoch #49, avg. train loss: 0.04994\n",
      "Step #64800, epoch #49, avg. train loss: 0.04580\n",
      "Step #64900, epoch #49, avg. train loss: 0.05308\n",
      "Step #65000, epoch #49, avg. train loss: 0.05145\n",
      "Step #65100, epoch #49, avg. train loss: 0.03818\n",
      "Step #65200, epoch #49, avg. train loss: 0.04365\n",
      "Step #65300, epoch #49, avg. train loss: 0.04482\n",
      "Step #65400, epoch #49, avg. train loss: 0.05002\n",
      "Step #65500, epoch #49, avg. train loss: 0.05878\n",
      "Step #65600, epoch #49, avg. train loss: 0.04830\n",
      "Step #65700, epoch #50, avg. train loss: 0.03984\n",
      "Step #65800, epoch #50, avg. train loss: 0.04680\n",
      "Step #65900, epoch #50, avg. train loss: 0.03507\n",
      "Step #66000, epoch #50, avg. train loss: 0.03751\n",
      "Step #66100, epoch #50, avg. train loss: 0.03878\n",
      "Step #66200, epoch #50, avg. train loss: 0.05448\n",
      "Step #66300, epoch #50, avg. train loss: 0.03630\n",
      "Step #66400, epoch #50, avg. train loss: 0.06209\n",
      "Step #66500, epoch #50, avg. train loss: 0.04529\n",
      "Step #66600, epoch #50, avg. train loss: 0.04724\n",
      "Step #66700, epoch #50, avg. train loss: 0.05193\n",
      "Step #66800, epoch #50, avg. train loss: 0.05418\n",
      "Step #66900, epoch #50, avg. train loss: 0.04725\n",
      "Step #67000, epoch #51, avg. train loss: 0.05263\n",
      "Step #67100, epoch #51, avg. train loss: 0.03221\n",
      "Step #67200, epoch #51, avg. train loss: 0.04221\n",
      "Step #67300, epoch #51, avg. train loss: 0.04465\n",
      "Step #67400, epoch #51, avg. train loss: 0.04650\n",
      "Step #67500, epoch #51, avg. train loss: 0.04639\n",
      "Step #67600, epoch #51, avg. train loss: 0.04300\n",
      "Step #67700, epoch #51, avg. train loss: 0.05258\n",
      "Step #67800, epoch #51, avg. train loss: 0.04123\n",
      "Step #67900, epoch #51, avg. train loss: 0.06402\n",
      "Step #68000, epoch #51, avg. train loss: 0.03505\n",
      "Step #68100, epoch #51, avg. train loss: 0.04518\n",
      "Step #68200, epoch #51, avg. train loss: 0.06375\n",
      "Step #68300, epoch #52, avg. train loss: 0.04503\n",
      "Step #68400, epoch #52, avg. train loss: 0.04590\n",
      "Step #68500, epoch #52, avg. train loss: 0.02498\n",
      "Step #68600, epoch #52, avg. train loss: 0.04298\n",
      "Step #68700, epoch #52, avg. train loss: 0.04716\n",
      "Step #68800, epoch #52, avg. train loss: 0.05188\n",
      "Step #68900, epoch #52, avg. train loss: 0.04496\n",
      "Step #69000, epoch #52, avg. train loss: 0.03480\n",
      "Step #69100, epoch #52, avg. train loss: 0.03968\n",
      "Step #69200, epoch #52, avg. train loss: 0.04959\n",
      "Step #69300, epoch #52, avg. train loss: 0.05618\n",
      "Step #69400, epoch #52, avg. train loss: 0.04439\n",
      "Step #69500, epoch #52, avg. train loss: 0.04610\n",
      "Step #69600, epoch #53, avg. train loss: 0.04935\n",
      "Step #69700, epoch #53, avg. train loss: 0.02928\n",
      "Step #69800, epoch #53, avg. train loss: 0.04564\n",
      "Step #69900, epoch #53, avg. train loss: 0.03348\n",
      "Step #70000, epoch #53, avg. train loss: 0.04127\n",
      "Step #70100, epoch #53, avg. train loss: 0.02829\n",
      "Step #70200, epoch #53, avg. train loss: 0.03964\n",
      "Step #70300, epoch #53, avg. train loss: 0.03630\n",
      "Step #70400, epoch #53, avg. train loss: 0.05706\n",
      "Step #70500, epoch #53, avg. train loss: 0.05752\n",
      "Step #70600, epoch #53, avg. train loss: 0.07270\n",
      "Step #70700, epoch #53, avg. train loss: 0.03395\n",
      "Step #70800, epoch #53, avg. train loss: 0.04384\n",
      "Step #70900, epoch #53, avg. train loss: 0.04483\n",
      "Step #71000, epoch #54, avg. train loss: 0.03439\n",
      "Step #71100, epoch #54, avg. train loss: 0.03197\n",
      "Step #71200, epoch #54, avg. train loss: 0.04602\n",
      "Step #71300, epoch #54, avg. train loss: 0.05578\n",
      "Step #71400, epoch #54, avg. train loss: 0.03777\n",
      "Step #71500, epoch #54, avg. train loss: 0.03889\n",
      "Step #71600, epoch #54, avg. train loss: 0.04512\n",
      "Step #71700, epoch #54, avg. train loss: 0.04439\n",
      "Step #71800, epoch #54, avg. train loss: 0.05032\n",
      "Step #71900, epoch #54, avg. train loss: 0.05164\n",
      "Step #72000, epoch #54, avg. train loss: 0.04929\n",
      "Step #72100, epoch #54, avg. train loss: 0.03547\n",
      "Step #72200, epoch #54, avg. train loss: 0.04432\n",
      "Step #72300, epoch #55, avg. train loss: 0.03674\n",
      "Step #72400, epoch #55, avg. train loss: 0.04292\n",
      "Step #72500, epoch #55, avg. train loss: 0.04509\n",
      "Step #72600, epoch #55, avg. train loss: 0.04164\n",
      "Step #72700, epoch #55, avg. train loss: 0.04329\n",
      "Step #72800, epoch #55, avg. train loss: 0.03091\n",
      "Step #72900, epoch #55, avg. train loss: 0.03745\n",
      "Step #73000, epoch #55, avg. train loss: 0.03863\n",
      "Step #73100, epoch #55, avg. train loss: 0.05034\n",
      "Step #73200, epoch #55, avg. train loss: 0.03857\n",
      "Step #73300, epoch #55, avg. train loss: 0.04454\n",
      "Step #73400, epoch #55, avg. train loss: 0.03817\n",
      "Step #73500, epoch #55, avg. train loss: 0.04348\n",
      "Step #73600, epoch #56, avg. train loss: 0.04431\n",
      "Step #73700, epoch #56, avg. train loss: 0.04703\n",
      "Step #73800, epoch #56, avg. train loss: 0.04016\n",
      "Step #73900, epoch #56, avg. train loss: 0.03862\n",
      "Step #74000, epoch #56, avg. train loss: 0.03798\n",
      "Step #74100, epoch #56, avg. train loss: 0.04596\n",
      "Step #74200, epoch #56, avg. train loss: 0.02665\n",
      "Step #74300, epoch #56, avg. train loss: 0.03539\n",
      "Step #74400, epoch #56, avg. train loss: 0.02855\n",
      "Step #74500, epoch #56, avg. train loss: 0.04600\n",
      "Step #74600, epoch #56, avg. train loss: 0.02633\n",
      "Step #74700, epoch #56, avg. train loss: 0.03771\n",
      "Step #74800, epoch #56, avg. train loss: 0.04777\n",
      "Step #74900, epoch #57, avg. train loss: 0.04607\n",
      "Step #75000, epoch #57, avg. train loss: 0.03946\n",
      "Step #75100, epoch #57, avg. train loss: 0.03418\n",
      "Step #75200, epoch #57, avg. train loss: 0.04969\n",
      "Step #75300, epoch #57, avg. train loss: 0.03720\n",
      "Step #75400, epoch #57, avg. train loss: 0.04645\n",
      "Step #75500, epoch #57, avg. train loss: 0.03263\n",
      "Step #75600, epoch #57, avg. train loss: 0.03988\n",
      "Step #75700, epoch #57, avg. train loss: 0.03793\n",
      "Step #75800, epoch #57, avg. train loss: 0.03181\n",
      "Step #75900, epoch #57, avg. train loss: 0.03613\n",
      "Step #76000, epoch #57, avg. train loss: 0.04726\n",
      "Step #76100, epoch #57, avg. train loss: 0.02946\n",
      "Step #76200, epoch #58, avg. train loss: 0.03091\n",
      "Step #76300, epoch #58, avg. train loss: 0.03182\n",
      "Step #76400, epoch #58, avg. train loss: 0.05218\n",
      "Step #76500, epoch #58, avg. train loss: 0.03873\n",
      "Step #76600, epoch #58, avg. train loss: 0.03202\n",
      "Step #76700, epoch #58, avg. train loss: 0.03884\n",
      "Step #76800, epoch #58, avg. train loss: 0.03660\n",
      "Step #76900, epoch #58, avg. train loss: 0.03836\n",
      "Step #77000, epoch #58, avg. train loss: 0.04479\n",
      "Step #77100, epoch #58, avg. train loss: 0.02887\n",
      "Step #77200, epoch #58, avg. train loss: 0.03421\n",
      "Step #77300, epoch #58, avg. train loss: 0.04234\n",
      "Step #77400, epoch #58, avg. train loss: 0.03673\n",
      "Step #77500, epoch #59, avg. train loss: 0.04308\n",
      "Step #77600, epoch #59, avg. train loss: 0.03828\n",
      "Step #77700, epoch #59, avg. train loss: 0.03697\n",
      "Step #77800, epoch #59, avg. train loss: 0.02946\n",
      "Step #77900, epoch #59, avg. train loss: 0.03510\n",
      "Step #78000, epoch #59, avg. train loss: 0.04573\n",
      "Step #78100, epoch #59, avg. train loss: 0.02840\n",
      "Step #78200, epoch #59, avg. train loss: 0.03329\n",
      "Step #78300, epoch #59, avg. train loss: 0.04282\n",
      "Step #78400, epoch #59, avg. train loss: 0.02722\n",
      "Step #78500, epoch #59, avg. train loss: 0.03339\n",
      "Step #78600, epoch #59, avg. train loss: 0.05558\n",
      "Step #78700, epoch #59, avg. train loss: 0.05601\n",
      "Step #78800, epoch #60, avg. train loss: 0.04379\n",
      "Step #78900, epoch #60, avg. train loss: 0.03632\n",
      "Step #79000, epoch #60, avg. train loss: 0.03099\n",
      "Step #79100, epoch #60, avg. train loss: 0.03692\n",
      "Step #79200, epoch #60, avg. train loss: 0.04340\n",
      "Step #79300, epoch #60, avg. train loss: 0.03460\n",
      "Step #79400, epoch #60, avg. train loss: 0.03878\n",
      "Step #79500, epoch #60, avg. train loss: 0.03247\n",
      "Step #79600, epoch #60, avg. train loss: 0.04423\n",
      "Step #79700, epoch #60, avg. train loss: 0.03177\n",
      "Step #79800, epoch #60, avg. train loss: 0.03345\n",
      "Step #79900, epoch #60, avg. train loss: 0.03086\n",
      "Step #80000, epoch #60, avg. train loss: 0.03144\n",
      "Step #80100, epoch #61, avg. train loss: 0.05467\n",
      "Step #80200, epoch #61, avg. train loss: 0.02978\n",
      "Step #80300, epoch #61, avg. train loss: 0.03419\n",
      "Step #80400, epoch #61, avg. train loss: 0.02267\n",
      "Step #80500, epoch #61, avg. train loss: 0.03803\n",
      "Step #80600, epoch #61, avg. train loss: 0.04037\n",
      "Step #80700, epoch #61, avg. train loss: 0.03718\n",
      "Step #80800, epoch #61, avg. train loss: 0.03962\n",
      "Step #80900, epoch #61, avg. train loss: 0.03445\n",
      "Step #81000, epoch #61, avg. train loss: 0.02954\n",
      "Step #81100, epoch #61, avg. train loss: 0.03555\n",
      "Step #81200, epoch #61, avg. train loss: 0.04055\n",
      "Step #81300, epoch #61, avg. train loss: 0.04021\n",
      "Step #81400, epoch #61, avg. train loss: 0.04778\n",
      "Step #81500, epoch #62, avg. train loss: 0.04877\n",
      "Step #81600, epoch #62, avg. train loss: 0.04348\n",
      "Step #81700, epoch #62, avg. train loss: 0.02691\n",
      "Step #81800, epoch #62, avg. train loss: 0.02489\n",
      "Step #81900, epoch #62, avg. train loss: 0.03599\n",
      "Step #82000, epoch #62, avg. train loss: 0.03074\n",
      "Step #82100, epoch #62, avg. train loss: 0.04363\n",
      "Step #82200, epoch #62, avg. train loss: 0.03507\n",
      "Step #82300, epoch #62, avg. train loss: 0.02993\n",
      "Step #82400, epoch #62, avg. train loss: 0.03872\n",
      "Step #82500, epoch #62, avg. train loss: 0.02380\n",
      "Step #82600, epoch #62, avg. train loss: 0.05083\n",
      "Step #82700, epoch #62, avg. train loss: 0.05349\n",
      "Step #82800, epoch #63, avg. train loss: 0.03415\n",
      "Step #82900, epoch #63, avg. train loss: 0.04133\n",
      "Step #83000, epoch #63, avg. train loss: 0.04185\n",
      "Step #83100, epoch #63, avg. train loss: 0.03730\n",
      "Step #83200, epoch #63, avg. train loss: 0.02543\n",
      "Step #83300, epoch #63, avg. train loss: 0.02805\n",
      "Step #83400, epoch #63, avg. train loss: 0.03993\n",
      "Step #83500, epoch #63, avg. train loss: 0.03663\n",
      "Step #83600, epoch #63, avg. train loss: 0.04218\n",
      "Step #83700, epoch #63, avg. train loss: 0.02620\n",
      "Step #83800, epoch #63, avg. train loss: 0.03182\n",
      "Step #83900, epoch #63, avg. train loss: 0.03870\n",
      "Step #84000, epoch #63, avg. train loss: 0.03171\n",
      "Step #84100, epoch #64, avg. train loss: 0.02838\n",
      "Step #84200, epoch #64, avg. train loss: 0.02920\n",
      "Step #84300, epoch #64, avg. train loss: 0.03759\n",
      "Step #84400, epoch #64, avg. train loss: 0.05374\n",
      "Step #84500, epoch #64, avg. train loss: 0.04016\n",
      "Step #84600, epoch #64, avg. train loss: 0.02791\n",
      "Step #84700, epoch #64, avg. train loss: 0.02134\n",
      "Step #84800, epoch #64, avg. train loss: 0.02721\n",
      "Step #84900, epoch #64, avg. train loss: 0.02964\n",
      "Step #85000, epoch #64, avg. train loss: 0.02703\n",
      "Step #85100, epoch #64, avg. train loss: 0.05595\n",
      "Step #85200, epoch #64, avg. train loss: 0.04019\n",
      "Step #85300, epoch #64, avg. train loss: 0.03722\n",
      "Step #85400, epoch #65, avg. train loss: 0.02909\n",
      "Step #85500, epoch #65, avg. train loss: 0.02609\n",
      "Step #85600, epoch #65, avg. train loss: 0.03275\n",
      "Step #85700, epoch #65, avg. train loss: 0.03514\n",
      "Step #85800, epoch #65, avg. train loss: 0.03304\n",
      "Step #85900, epoch #65, avg. train loss: 0.02404\n",
      "Step #86000, epoch #65, avg. train loss: 0.03904\n",
      "Step #86100, epoch #65, avg. train loss: 0.04006\n",
      "Step #86200, epoch #65, avg. train loss: 0.03990\n",
      "Step #86300, epoch #65, avg. train loss: 0.03475\n",
      "Step #86400, epoch #65, avg. train loss: 0.03254\n",
      "Step #86500, epoch #65, avg. train loss: 0.03195\n",
      "Step #86600, epoch #65, avg. train loss: 0.03946\n",
      "Step #86700, epoch #66, avg. train loss: 0.03861\n",
      "Step #86800, epoch #66, avg. train loss: 0.02517\n",
      "Step #86900, epoch #66, avg. train loss: 0.02704\n",
      "Step #87000, epoch #66, avg. train loss: 0.03448\n",
      "Step #87100, epoch #66, avg. train loss: 0.02357\n",
      "Step #87200, epoch #66, avg. train loss: 0.03628\n",
      "Step #87300, epoch #66, avg. train loss: 0.02372\n",
      "Step #87400, epoch #66, avg. train loss: 0.03478\n",
      "Step #87500, epoch #66, avg. train loss: 0.03397\n",
      "Step #87600, epoch #66, avg. train loss: 0.04059\n",
      "Step #87700, epoch #66, avg. train loss: 0.04209\n",
      "Step #87800, epoch #66, avg. train loss: 0.03966\n",
      "Step #87900, epoch #66, avg. train loss: 0.03084\n",
      "Step #88000, epoch #67, avg. train loss: 0.03240\n",
      "Step #88100, epoch #67, avg. train loss: 0.03398\n",
      "Step #88200, epoch #67, avg. train loss: 0.03795\n",
      "Step #88300, epoch #67, avg. train loss: 0.03472\n",
      "Step #88400, epoch #67, avg. train loss: 0.02458\n",
      "Step #88500, epoch #67, avg. train loss: 0.03741\n",
      "Step #88600, epoch #67, avg. train loss: 0.03210\n",
      "Step #88700, epoch #67, avg. train loss: 0.02411\n",
      "Step #88800, epoch #67, avg. train loss: 0.03725\n",
      "Step #88900, epoch #67, avg. train loss: 0.03361\n",
      "Step #89000, epoch #67, avg. train loss: 0.03452\n",
      "Step #89100, epoch #67, avg. train loss: 0.04190\n",
      "Step #89200, epoch #67, avg. train loss: 0.04278\n",
      "Step #89300, epoch #68, avg. train loss: 0.03095\n",
      "Step #89400, epoch #68, avg. train loss: 0.02677\n",
      "Step #89500, epoch #68, avg. train loss: 0.02461\n",
      "Step #89600, epoch #68, avg. train loss: 0.04281\n",
      "Step #89700, epoch #68, avg. train loss: 0.02762\n",
      "Step #89800, epoch #68, avg. train loss: 0.02421\n",
      "Step #89900, epoch #68, avg. train loss: 0.02112\n",
      "Step #90000, epoch #68, avg. train loss: 0.03349\n",
      "Step #90100, epoch #68, avg. train loss: 0.02696\n",
      "Step #90200, epoch #68, avg. train loss: 0.03778\n",
      "Step #90300, epoch #68, avg. train loss: 0.04379\n",
      "Step #90400, epoch #68, avg. train loss: 0.03121\n",
      "Step #90500, epoch #68, avg. train loss: 0.04013\n",
      "Step #90600, epoch #69, avg. train loss: 0.03128\n",
      "Step #90700, epoch #69, avg. train loss: 0.02119\n",
      "Step #90800, epoch #69, avg. train loss: 0.02735\n",
      "Step #90900, epoch #69, avg. train loss: 0.02732\n",
      "Step #91000, epoch #69, avg. train loss: 0.02431\n",
      "Step #91100, epoch #69, avg. train loss: 0.03567\n",
      "Step #91200, epoch #69, avg. train loss: 0.03244\n",
      "Step #91300, epoch #69, avg. train loss: 0.03106\n",
      "Step #91400, epoch #69, avg. train loss: 0.06025\n",
      "Step #91500, epoch #69, avg. train loss: 0.03147\n",
      "Step #91600, epoch #69, avg. train loss: 0.03180\n",
      "Step #91700, epoch #69, avg. train loss: 0.02766\n",
      "Step #91800, epoch #69, avg. train loss: 0.04202\n",
      "Step #91900, epoch #69, avg. train loss: 0.03553\n",
      "Step #92000, epoch #70, avg. train loss: 0.03552\n",
      "Step #92100, epoch #70, avg. train loss: 0.03597\n",
      "Step #92200, epoch #70, avg. train loss: 0.03542\n",
      "Step #92300, epoch #70, avg. train loss: 0.03740\n",
      "Step #92400, epoch #70, avg. train loss: 0.03891\n",
      "Step #92500, epoch #70, avg. train loss: 0.02118\n",
      "Step #92600, epoch #70, avg. train loss: 0.02643\n",
      "Step #92700, epoch #70, avg. train loss: 0.03461\n",
      "Step #92800, epoch #70, avg. train loss: 0.02061\n",
      "Step #92900, epoch #70, avg. train loss: 0.04952\n",
      "Step #93000, epoch #70, avg. train loss: 0.03185\n",
      "Step #93100, epoch #70, avg. train loss: 0.02887\n",
      "Step #93200, epoch #70, avg. train loss: 0.02867\n",
      "Step #93300, epoch #71, avg. train loss: 0.03014\n",
      "Step #93400, epoch #71, avg. train loss: 0.03862\n",
      "Step #93500, epoch #71, avg. train loss: 0.02384\n",
      "Step #93600, epoch #71, avg. train loss: 0.03031\n",
      "Step #93700, epoch #71, avg. train loss: 0.03161\n",
      "Step #93800, epoch #71, avg. train loss: 0.02124\n",
      "Step #93900, epoch #71, avg. train loss: 0.03150\n",
      "Step #94000, epoch #71, avg. train loss: 0.03007\n",
      "Step #94100, epoch #71, avg. train loss: 0.02839\n",
      "Step #94200, epoch #71, avg. train loss: 0.04713\n",
      "Step #94300, epoch #71, avg. train loss: 0.02674\n",
      "Step #94400, epoch #71, avg. train loss: 0.04276\n",
      "Step #94500, epoch #71, avg. train loss: 0.05005\n",
      "Step #94600, epoch #72, avg. train loss: 0.02325\n",
      "Step #94700, epoch #72, avg. train loss: 0.02990\n",
      "Step #94800, epoch #72, avg. train loss: 0.03129\n",
      "Step #94900, epoch #72, avg. train loss: 0.02171\n",
      "Step #95000, epoch #72, avg. train loss: 0.02705\n",
      "Step #95100, epoch #72, avg. train loss: 0.02237\n",
      "Step #95200, epoch #72, avg. train loss: 0.03106\n",
      "Step #95300, epoch #72, avg. train loss: 0.04680\n",
      "Step #95400, epoch #72, avg. train loss: 0.02444\n",
      "Step #95500, epoch #72, avg. train loss: 0.03367\n",
      "Step #95600, epoch #72, avg. train loss: 0.03404\n",
      "Step #95700, epoch #72, avg. train loss: 0.03003\n",
      "Step #95800, epoch #72, avg. train loss: 0.03230\n",
      "Step #95900, epoch #73, avg. train loss: 0.02914\n",
      "Step #96000, epoch #73, avg. train loss: 0.02073\n",
      "Step #96100, epoch #73, avg. train loss: 0.03835\n",
      "Step #96200, epoch #73, avg. train loss: 0.02833\n",
      "Step #96300, epoch #73, avg. train loss: 0.02830\n",
      "Step #96400, epoch #73, avg. train loss: 0.02544\n",
      "Step #96500, epoch #73, avg. train loss: 0.03288\n",
      "Step #96600, epoch #73, avg. train loss: 0.03662\n",
      "Step #96700, epoch #73, avg. train loss: 0.03407\n",
      "Step #96800, epoch #73, avg. train loss: 0.02085\n",
      "Step #96900, epoch #73, avg. train loss: 0.02302\n",
      "Step #97000, epoch #73, avg. train loss: 0.02993\n",
      "Step #97100, epoch #73, avg. train loss: 0.03130\n",
      "Step #97200, epoch #74, avg. train loss: 0.03178\n",
      "Step #97300, epoch #74, avg. train loss: 0.02601\n",
      "Step #97400, epoch #74, avg. train loss: 0.03154\n",
      "Step #97500, epoch #74, avg. train loss: 0.02974\n",
      "Step #97600, epoch #74, avg. train loss: 0.03051\n",
      "Step #97700, epoch #74, avg. train loss: 0.03890\n",
      "Step #97800, epoch #74, avg. train loss: 0.02356\n",
      "Step #97900, epoch #74, avg. train loss: 0.01793\n",
      "Step #98000, epoch #74, avg. train loss: 0.03348\n",
      "Step #98100, epoch #74, avg. train loss: 0.03383\n",
      "Step #98200, epoch #74, avg. train loss: 0.04278\n",
      "Step #98300, epoch #74, avg. train loss: 0.02535\n",
      "Step #98400, epoch #74, avg. train loss: 0.03730\n",
      "Step #98500, epoch #75, avg. train loss: 0.02591\n",
      "Step #98600, epoch #75, avg. train loss: 0.04722\n",
      "Step #98700, epoch #75, avg. train loss: 0.03166\n",
      "Step #98800, epoch #75, avg. train loss: 0.04033\n",
      "Step #98900, epoch #75, avg. train loss: 0.03690\n",
      "Step #99000, epoch #75, avg. train loss: 0.03722\n",
      "Step #99100, epoch #75, avg. train loss: 0.02270\n",
      "Step #99200, epoch #75, avg. train loss: 0.03111\n",
      "Step #99300, epoch #75, avg. train loss: 0.02401\n",
      "Step #99400, epoch #75, avg. train loss: 0.03297\n",
      "Step #99500, epoch #75, avg. train loss: 0.02492\n",
      "Step #99600, epoch #75, avg. train loss: 0.02391\n",
      "Step #99700, epoch #75, avg. train loss: 0.02639\n",
      "Step #99800, epoch #76, avg. train loss: 0.02138\n",
      "Step #99900, epoch #76, avg. train loss: 0.01674\n",
      "Step #100000, epoch #76, avg. train loss: 0.02449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorFlowDNNClassifier(batch_size=32, class_weight=None, clip_gradients=5.0,\n",
       "            config=None, continue_training=False, dropout=None,\n",
       "            hidden_units=[100, 50], learning_rate=0.1, n_classes=10,\n",
       "            optimizer='Adagrad', steps=100000, verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "x_train = x_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "model_nn = skflow.TensorFlowDNNClassifier(hidden_units=[100,50], n_classes=10, steps=100000)\n",
    "model_nn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the accuracy of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94135714285714289"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99328571428571433"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nn.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submit(model,x_test, name):\n",
    "    # predict y using the test set\n",
    "    y_test = model.predict(x_test) \n",
    "    # kaggles requires the submission to include an index column         \n",
    "    index = np.arange(1,len(x_test)+1, dtype=int)  \n",
    "    # merging the y_test and index columns        \n",
    "    y_test = np.column_stack((index,y_test))\n",
    "    # convert to pandas dataframe\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    # headers required for the submission                          \n",
    "    y_test.columns = ['ImageId','Label']    \n",
    "    # write the data to csv file in the directory    \n",
    "    y_test = y_test.astype('int')\n",
    "    y_test.to_csv(\"\".join([name,'.csv']), index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit(model_log,x_test,'model_log')\n",
    "submit(model_rf,x_test,'model_rf')\n",
    "submit(model_nn,x_test,'model_nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "| Model               | Test Accuracy Score |\n",
    "|---------------------|---------------------|\n",
    "| Logistic Regression | 0.91771             |\n",
    "| Random Forests      | 0.96800             |\n",
    "| Neural Networks     | 0.94900             |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
